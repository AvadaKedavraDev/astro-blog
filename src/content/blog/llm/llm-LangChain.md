---
title: "LangChainæ‰‹å†Œ"
pubDate: 2024-02-15
description: "LangChainé€ŸæŸ¥"
tags: ["LLM", "æ•™ç¨‹", "LangChin"]
#coverImage: "/images/docker-cover.jpg"
readingTime: 15
pinned: true

# ç³»åˆ—æ–‡ç« 
series:
  name: "LLM å…¥é—¨åˆ°å®æˆ˜"
  order: 1
---
# LangChain å…¥é—¨

> [!info] ä»€ä¹ˆæ˜¯ LangChainï¼Ÿ
> LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰åº”ç”¨çš„ Python/JS æ¡†æ¶ï¼Œå®ƒæä¾›äº†ï¼š
> - æ ‡å‡†åŒ–æ¥å£ï¼šç»Ÿä¸€è°ƒç”¨ OpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰
> - é“¾å¼ç»„åˆï¼šå°†å¤šä¸ªç»„ä»¶ï¼ˆPrompt â†’ LLM â†’ è¾“å‡ºè§£æï¼‰ä¸²è”
> - è®°å¿†ç®¡ç†ï¼šæ”¯æŒå¯¹è¯å†å²çš„çŸ­æœŸ/é•¿æœŸè®°å¿†
> - å·¥å…·è°ƒç”¨ï¼šè®© LLM ä½¿ç”¨å¤–éƒ¨ APIã€æœç´¢ã€è®¡ç®—ç­‰å·¥å…·ï¼ˆAgentï¼‰

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/get_started/introduction)

> [!tip] å®‰è£…ç¯å¢ƒ
> ```bash # åŸºç¡€å®‰è£…
> pip install langchain
>
> # å¸¸è§æ¨¡å‹ providerï¼ˆä»»é€‰å…¶ä¸€æˆ–å¤šä¸ªï¼‰
> pip install langchain-openai    # OpenAI GPT
> pip install langchain-anthropic # Claude
> pip install langchain-ollama    # æœ¬åœ°æ¨¡å‹
>
> # å…¶ä»–å¸¸ç”¨ä¾èµ–
> pip install langchain-community  # ç¬¬ä¸‰æ–¹é›†æˆ
> pip install langchainhub        # Prompt æ¨¡æ¿åº“ 
>```

## 1. Hello LangChain åŸºç¡€ç¤ºä¾‹

### 1.1 ç›´æ¥è°ƒç”¨

```python {4-5}
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage

llm = ChatOllama(model="qwen3:8b", temperature=0.8)
response = llm.invoke([HumanMessage(content="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±")])

print(response.content)
```
ç»“æœï¼š
```console
ä½ å¥½ï¼æˆ‘å«é€šä¹‰åƒé—®ï¼Œæ˜¯é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘çš„çŸ¥è¯†å’Œèƒ½åŠ›æ¥æºäºå¤§é‡çš„æ–‡æœ¬æ•°æ®è®­ç»ƒï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·å®Œæˆå¤šç§ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

1. **å›ç­”é—®é¢˜**ï¼šæ— è®ºæ˜¯æ—¥å¸¸ç–‘é—®è¿˜æ˜¯ä¸“ä¸šé¢†åŸŸçš„é—®é¢˜ï¼Œæˆ‘éƒ½å¯ä»¥æä¾›å‚è€ƒç­”æ¡ˆã€‚
2. **åˆ›ä½œæ–‡å­—**ï¼šåŒ…æ‹¬å†™æ•…äº‹ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€å†™ä»£ç æ³¨é‡Šç­‰ã€‚
3. **é€»è¾‘æ¨ç†**ï¼šå¸®åŠ©åˆ†æé—®é¢˜ã€è§£å†³æ•°å­¦é¢˜ã€ç¼–ç¨‹é—®é¢˜ç­‰ã€‚
4. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæˆ‘æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ©æ–‡ç­‰ã€‚
5. **æ—¥å¸¸äº¤æµ**ï¼šå¯ä»¥è¿›è¡Œé—²èŠã€æä¾›å»ºè®®ã€åˆ†äº«çŸ¥è¯†ç­‰ã€‚

æˆ‘çš„ç›®æ ‡æ˜¯æˆä¸ºç”¨æˆ·åœ¨å­¦ä¹ ã€å·¥ä½œå’Œç”Ÿæ´»ä¸­çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¸®åŠ©ä»–ä»¬æ›´é«˜æ•ˆåœ°è·å–ä¿¡æ¯å’Œè§£å†³é—®é¢˜ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š
```

### 1.2 ä½¿ç”¨ Prompt æ¨¡æ¿è°ƒç”¨

```python {5-8,14,17-20}
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

# åˆ›å»ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„{role}ï¼Œç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"),
    ("human", "{question}")
])

# æ¨¡å‹
llm = ChatOllama(model="qwen3:8b", temperature=0.8)

# ç»„åˆæˆ Chainï¼ˆç®¡é“æ“ä½œç¬¦ |ï¼‰
chain = prompt | llm

# è¿è¡Œ
result = chain.invoke({
    "role": "Python å¯¼å¸ˆ",
    "question": "æœ‰å“ªäº›åŸºç¡€å˜é‡ï¼Ÿ"
})

print(result.content)
```

## 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 2.1 Chainï¼ˆé“¾ï¼‰ 

```python {9-13,15}
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

# æ¨¡å‹
llm = ChatOllama(model="qwen3:8b", temperature=0.8)

# å®Œæ•´é“¾æ¡ï¼šPrompt -> LLM -> å­—ç¬¦ä¸²è¾“å‡º
chain = (
    ChatPromptTemplate.from_template("ç”¨ä¸€å¥è¯è§£é‡Š{concept}")
    | llm
    | StrOutputParser()  # æå–çº¯æ–‡æœ¬
)

output = chain.invoke({"concept": "é€’å½’å‡½æ•°"})

print(output)
# é€’å½’å‡½æ•°æ˜¯ä¸€ç§é€šè¿‡è°ƒç”¨è‡ªèº«æ¥è§£å†³é—®é¢˜çš„æ–¹æ³•ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ªç»ˆæ­¢æ¡ä»¶ä»¥é˜²æ­¢æ— é™å¾ªç¯ã€‚
```

### 2.1 Memoryï¼ˆè®°å¿†ï¼‰

```python {12-16,25-29,31-37}
from langchain.chains import ConversationChain
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_ollama import ChatOllama

# 1. åˆå§‹åŒ–æ¨¡å‹
llm = ChatOllama(model="qwen3:8b", temperature=0.8)

# 2. å®šä¹‰æç¤ºæ¨¡æ¿ï¼ˆä½¿ç”¨ MessagesPlaceholder æ³¨å…¥å†å²ï¼‰
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"),
    MessagesPlaceholder(variable_name="history"),  # å†å²æ¶ˆæ¯å ä½ç¬¦
    ("human", "{input}")
])

# 3. æ„å»ºåŸºç¡€ chain
chain = prompt | llm | StrOutputParser()

# 4. å­˜å‚¨å†å²ä¼šè¯
store = {}

# åˆ›å»ºå¯¹è¯é“¾
def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    """æ ¹æ® session_id è·å–æˆ–åˆ›å»ºå¯¹è¯å†å²"""
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

# 5. åŒ…è£…ä¸ºå¸¦å†å²è®°å½•çš„ Runnable
conversation = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",  # è¾“å…¥æ¶ˆæ¯çš„é”®å
    history_messages_key="history"  # å†å²æ¶ˆæ¯çš„é”®åï¼ˆä¸ MessagesPlaceholder å¯¹åº”ï¼‰
)

# 6. ä½¿ç”¨æ–¹å¼ï¼ˆå¿…é¡»ä¼ å…¥ config æŒ‡å®š session_idï¼‰
config = {"configurable": {"session_id": "user_123"}}

# ç¬¬ä¸€æ¬¡å¯¹è¯
response1 = conversation.invoke(
    {"input": "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰"},
    config=config
)

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼ˆèƒ½è®°ä½åå­—ï¼‰
response2 = conversation.invoke(
    {"input": "æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ"},
    config=config
)

print(response2)
# ä½ å¥½ï¼Œå¼ ä¸‰ï¼æˆ‘åˆšåˆšç¡®è®¤äº†ä½ çš„åå­—ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ
```

### 2.3 Agentï¼ˆä»£ç†ï¼‰

è®© LLM è‡ªä¸»å†³ç­–å¹¶ä½¿ç”¨å·¥å…·ï¼š

```python
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate
from langchain_ollama import ChatOllama
import os

# è®¾ç½® Tavily API Keyï¼ˆhttps://tavily.com å…è´¹æ³¨å†Œè·å–ï¼‰
os.environ["TAVILY_API_KEY"] = "tvly-dev-key"

# åˆå§‹åŒ–å·¥å…·ï¼ˆä¸­æ–‡æè¿°å¸®åŠ©æ¨¡å‹ç†è§£å·¥å…·ç”¨é€”ï¼‰
tools = [
    TavilySearchResults(
        max_results=3,
        description="æœç´¢å¼•æ“ï¼Œç”¨äºæŸ¥æ‰¾å®æ—¶ä¿¡æ¯ã€æ–°é—»ã€ç™¾ç§‘çŸ¥è¯†ç­‰ç½‘ç»œå†…å®¹"
    )
]

# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
llm = ChatOllama(model="qwen3:8b", temperature=0)

# ä¸­æ–‡ ReAct æç¤ºæ¨¡æ¿
# æ³¨æ„ï¼šThought/Action/Action Input/Observation/Final Answer å¿…é¡»ä¿æŒè‹±æ–‡
# è¿™äº›æ˜¯ Agent çš„è§£ææ ‡è®°ï¼Œæ”¹ä¸ºä¸­æ–‡ä¼šå¯¼è‡´è¯†åˆ«å¤±è´¥
template = """å°½å¯èƒ½å›ç­”ç”¨æˆ·é—®é¢˜ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š

{tools}

ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š

Question: éœ€è¦å›ç­”çš„è¾“å…¥é—®é¢˜
Thought: æ€è€ƒåº”è¯¥é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨
Action: è¦æ‰§è¡Œçš„è¡ŒåŠ¨ï¼Œå¿…é¡»æ˜¯ [{tool_names}] ä¹‹ä¸€
Action Input: è¡ŒåŠ¨çš„è¾“å…¥ï¼ˆç›´æ¥çš„é—®é¢˜æˆ–å…³é”®è¯ï¼‰
Observation: è¡ŒåŠ¨æ‰§è¡Œåçš„è¿”å›ç»“æœ
...ï¼ˆThought/Action/Action Input/Observation å¯ä»¥é‡å¤å¤šæ¬¡ï¼‰
Thought: æˆ‘ç°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
Final Answer: å¯¹åŸå§‹é—®é¢˜çš„æœ€ç»ˆå›ç­”ï¼ˆç”¨ä¸­æ–‡å›ç­”ï¼‰

é‡è¦æç¤ºï¼šå½“å‰å¹´ä»½æ˜¯2026å¹´ï¼Œå¦‚æœé—®é¢˜æ¶‰åŠæœ€æ–°äº‹ä»¶ï¼Œå¿…é¡»ä½¿ç”¨å·¥å…·æœç´¢ï¼Œä¸è¦ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†ã€‚

å¼€å§‹å›ç­”ï¼

Question: {input}
Thought:{agent_scratchpad}"""

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = PromptTemplate.from_template(template)

# åˆ›å»º Agent
agent = create_react_agent(llm, tools, prompt)

# åˆ›å»ºæ‰§è¡Œå™¨
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=False,  # å…³é—­è¯¦ç»†æ—¥å¿—é¿å…ä¹±ç 
    handle_parsing_errors=True,  # è‡ªåŠ¨å¤„ç†è§£æé”™è¯¯
    max_iterations=5  # æœ€å¤šæ€è€ƒ5æ­¥ï¼Œé˜²æ­¢æ­»å¾ªç¯
)

# è¿è¡ŒæŸ¥è¯¢
if __name__ == "__main__":
    question = "2024å¹´æ˜¥æ™šèŠ‚ç›®å•ï¼Ÿ"
    print(f"é—®é¢˜ï¼š{question}")
    print("-" * 50)

    try:
        response = agent_executor.invoke({"input": question})
        print(f"ç­”æ¡ˆï¼š{response['output']}")
    except Exception as e:
        print(f"æ‰§è¡Œå‡ºé”™ï¼š{e}")
```

<details>
<summary>ç»“æœå¦‚ä¸‹ï¼Œå±•å¼€</summary>

```
é—®é¢˜ï¼š2024å¹´æ˜¥æ™šèŠ‚ç›®å•ï¼Ÿ
--------------------------------------------------
ç­”æ¡ˆï¼š2024å¹´ä¸­å¤®å¹¿æ’­ç”µè§†æ€»å°æ˜¥èŠ‚è”æ¬¢æ™šä¼šèŠ‚ç›®å•å¦‚ä¸‹ï¼ˆæŒ‰å®é™…é¡ºåºæ•´ç†ï¼‰ï¼š

**ä¸»è¦èŠ‚ç›®ï¼š**
1. **å¼€åœºèŠ‚ç›®**ã€Šé¼“èˆé¾™è…¾ã€‹- è®¸å¨£ã€åˆ˜ä½©çªã€è‹æœ‰æœ‹ã€é™ˆé¾™ã€æ²™æº¢ã€ç‹é˜³ã€èƒ¡æ­Œã€å”å«£ã€é»„è½©ã€è¾›èŠ·è•¾ã€æ¨å¹‚ã€æ¯›æ™“å½¤ã€å®‹è½¶ã€ä¾¯é›¯å…ƒã€é»„æ›¦å½¦ã€å²ä»“åº“ã€è°­é‡‘æ·˜ã€åˆ˜å½±ç­‰
2. **æ­Œæ›²**ã€Šå˜¿ å°‘å¹´ã€‹- ç‹å‡¯ã€é™ˆä¼Ÿéœ†ã€æœ±ä¸€é¾™
3. **ç›¸å£°**ã€Šæˆ‘è¦ä¸ä¸€æ ·ã€‹- å²³äº‘é¹ã€å­™è¶Š
4. **åˆ›æ„å¹´ä¿—ç§€**ã€Šåˆ«å¼€ç”Ÿé¢ã€‹- é»‘æ’’ä¹é˜Ÿã€æäº®èŠ‚ã€ç™½ä¸¾çº²ã€æ›¾èˆœæ™ï¼ˆç‰¹é‚€ä¹é˜Ÿï¼šèƒ¡æ­Œã€å”å«£ã€è¾›èŠ·è•¾ã€é™ˆé¾™ï¼‰
....
```

</details>


### 2.4 å®Œæ•´RAGé—®ç­”demo

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama, OllamaEmbeddings

# 1. åŠ è½½ & åˆ‡åˆ†ï¼ˆä¸å˜ï¼‰
loader = TextLoader("company_manual.txt", encoding="utf-8")

documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)

# 2. å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨æœ¬åœ° Ollama åµŒå…¥ï¼Œæ— éœ€ OpenAI API Keyï¼‰
embeddings = OllamaEmbeddings(model="shaw/dmeta-embedding-zh:latest")
vectorstore = FAISS.from_documents(texts, embeddings)

# 3. æ„å»º LCEL RAG é“¾
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¸çŸ¥é“ï¼Œå°±è¯´"æ ¹æ®èµ„æ–™æ— æ³•ç¡®å®š"ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

def format_docs(docs):
    """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
    return "\n\n".join(f"[æ¥æº{i+1}] {doc.page_content}"
                      for i, doc in enumerate(docs))

# æ„å»ºé“¾ï¼šè¾“å…¥ â†’ æ£€ç´¢+æ ¼å¼åŒ– â†’ å¡«å……æ¨¡æ¿ â†’ LLM â†’ è¾“å‡º
rag_chain = (
    {
        "context": retriever | format_docs,  # æ£€ç´¢å¹¶æ ¼å¼åŒ–æ–‡æ¡£
        "question": RunnablePassthrough()     # ç›´æ¥ä¼ é€’ç”¨æˆ·é—®é¢˜
    }
    | prompt
    | ChatOllama(model="qwen3:8b")
    | StrOutputParser()
)

# 4. æé—®ï¼ˆæ–°ç‰ˆç”¨ invokeï¼Œä¸å†ä¼ å­—å…¸ï¼‰
result = rag_chain.invoke("å…¬å¸çš„å¹´å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print(result)

# 5. å¦‚éœ€æŸ¥çœ‹æ¥æºï¼ˆæ‰‹åŠ¨è·å–ï¼‰
docs = retriever.invoke("å…¬å¸çš„å¹´å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print("\nå‚è€ƒç‰‡æ®µï¼š")
for i, doc in enumerate(docs, 1):
    print(f"{i}. {doc.page_content[:100]}...")
```


<details>
<summary>ç»“æœå¦‚ä¸‹ï¼Œå±•å¼€</summary>


```
å…¬å¸çš„å¹´å‡æ”¿ç­–å¦‚ä¸‹ï¼š

1. **å¹´å‡å¤©æ•°**æ ¹æ®å…¥èŒå¹´é™ç¡®å®šï¼š
   - å…¥èŒæ»¡1å¹´ä¸æ»¡3å¹´ï¼šæ¯å¹´5å¤©å¸¦è–ªå¹´å‡  
   - å…¥èŒæ»¡3å¹´ä¸æ»¡5å¹´ï¼šæ¯å¹´7å¤©å¸¦è–ªå¹´å‡  
   - å…¥èŒæ»¡5å¹´ä¸æ»¡10å¹´ï¼šæ¯å¹´10å¤©å¸¦è–ªå¹´å‡  
   - å…¥èŒæ»¡10å¹´ä»¥ä¸Šï¼šæ¯å¹´15å¤©å¸¦è–ªå¹´å‡  

2. **å¹´å‡ä½¿ç”¨è§„åˆ™**ï¼š
   - å¹´å‡å¯ç´¯ç§¯è‡³æ¬¡å¹´3æœˆ31æ—¥å‰ä½¿ç”¨ï¼Œé€¾æœŸæ¸…é›¶ã€‚  
   - ç”³è¯·å¹´å‡éœ€æå‰3ä¸ªå·¥ä½œæ—¥é€šè¿‡OAç³»ç»Ÿæäº¤ç”³è¯·ï¼Œå¹¶ç»ç›´æ¥ä¸Šçº§å®¡æ‰¹åæ–¹å¯ä¼‘å‡ã€‚

3. **å…¶ä»–å‡æœŸ**æŒ‰å›½å®¶è§„å®šæ‰§è¡Œï¼š
   - **å©šå‡**ï¼š3å¤©ï¼Œæ™šå©šè€…å¯äº«å—15å¤©ï¼ˆå«å‘¨æœ«ï¼‰ã€‚  
   - **äº§å‡**ï¼š158å¤©ï¼ˆå«å›½å®¶è§„å®šçš„98å¤©åŸºç¡€äº§å‡å’Œ60å¤©å»¶é•¿äº§å‡ï¼‰ã€‚  
   - **ç”·å‘˜å·¥é™ªäº§å‡**ï¼š15å¤©ã€‚  

ï¼ˆä¾æ®æ¥æº1ï¼‰

å‚è€ƒç‰‡æ®µï¼š
1. ã€ç§‘æŠ€æœ‰é™å…¬å¸å‘˜å·¥æ‰‹å†Œã€‘

ç¬¬ä¸€ç« ï¼šè€ƒå‹¤ä¸ä¼‘å‡åˆ¶åº¦

1.1 å·¥ä½œæ—¶é—´
å…¬å¸å®è¡Œå¼¹æ€§å·¥ä½œåˆ¶ï¼Œæ ¸å¿ƒå·¥ä½œæ—¶é—´ä¸ºä¸Šåˆ10:00è‡³ä¸‹åˆ4:00ã€‚æ ‡å‡†å·¥ä½œæ—¶é•¿ä¸ºæ¯æ—¥8å°æ—¶ï¼Œæ¯å‘¨40å°æ—¶ã€‚
ä¸Šç­æ—¶é—´å¯åœ¨ä¸Šåˆ8:3...
2. 6.1 åŠå…¬åŒºåŸŸç®¡ç†
- åŠå…¬ä½ä¿æŒæ•´æ´ï¼Œä¸‹ç­æ—¶å…³é—­ç”µè„‘å’Œæ˜¾ç¤ºå™¨
- ä¼šè®®å®¤ä½¿ç”¨åæ¢å¤åŸçŠ¶ï¼Œç™½æ¿æ“¦æ‹­å¹²å‡€
- èŒ¶æ°´é—´ç¦æ­¢è®¨è®ºæ¶‰å¯†ä¿¡æ¯
- è®¿å®¢éœ€åœ¨å‰å°ç™»è®°å¹¶ç”±å‘˜å·¥å…¨ç¨‹é™ªåŒ

6.2 ç€è£…è¦æ±‚
å‘¨ä¸€è‡³å‘¨...
3. ç¬¬ä¸‰ç« ï¼šæŠ¥é”€ä¸è´¢åŠ¡åˆ¶åº¦

3.1 å·®æ—…æŠ¥é”€æ ‡å‡†
å›½å†…å‡ºå·®ä½å®¿æ ‡å‡†ï¼š
- ä¸€çº¿åŸå¸‚ï¼ˆåŒ—ä¸Šå¹¿æ·±ï¼‰ï¼šæ¯æ™šä¸è¶…è¿‡500å…ƒ
- äºŒçº¿åŸå¸‚ï¼šæ¯æ™šä¸è¶…è¿‡400å…ƒ
- ä¸‰çº¿åŸå¸‚ï¼šæ¯æ™šä¸è¶…è¿‡300å…ƒ

äº¤é€šæ ‡å‡†ï¼š
- ...
```


</details>
